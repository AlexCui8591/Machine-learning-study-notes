# This is a note focusing on machine learning enigeering 
This project covers a lot of machine learning knowledge and project content in the learning process, learning by doing. Any comments and discussions are welcome!
### So let's start with the simplest neural network below!

## Steps in Writing a Neural Network

When building a neural network from scratch, the process can be divided into several key steps:

1. **Define Activation Functions**  
   Implement common activation functions (e.g., Sigmoid, ReLU, Tanh) that introduce non-linearity into the model.

## Activation Functions

Activation functions introduce non-linearity into neural networks and help determine whether a neuron should be activated.  
They map input signals into specific ranges (e.g., probabilities or bounded values), making deep learning models expressive and powerful.

---

### 1. Sigmoid
**Formula:**  
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

- Output range: (0, 1)  
- Commonly used in **binary classification**.  
- Limitation: suffers from the **vanishing gradient problem**.

<p align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg" width="400"/>
</p>

---

### 2. Tanh
**Formula:**  
\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]

- Output range: (-1, 1)  
- Zero-centered, better than sigmoid for convergence.  
- Still prone to **vanishing gradients**.

<p align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/c/cb/Activation_tanh.svg" width="400"/>
</p>

---

### 3. ReLU (Rectified Linear Unit)
**Formula:**  
\[
f(x) = \max(0, x)
\]

- Keeps positive values unchanged, replaces negatives with 0.  
- Helps **mitigate vanishing gradients** and speeds up training.  
- Limitation: may lead to **dead neurons** (outputs stuck at 0).

<p align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg" width="400"/>
</p>

---

### 4. Leaky ReLU
**Formula:**  
\[
f(x) = 
\begin{cases} 
x & \text{if } x \geq 0 \\ 
\alpha x & \text{if } x < 0 
\end{cases}
\]

- Introduces a small slope (\(\alpha \approx 0.01\)) for negative values.  
- Prevents the **dead neuron problem** in ReLU.  

<p align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/a/ae/Activation_prelu.svg" width="400"/>
</p>

---

### 5. ELU (Exponential Linear Unit)
**Formula:**  
\[
f(x) = 
\begin{cases} 
x & \text{if } x \geq 0 \\ 
\alpha (e^x - 1) & \text{if } x < 0 
\end{cases}
\]

- Smooths the negative part compared to ReLU.  
- Helps reduce bias shift and improve training.



---

### 6. Softmax
**Formula:**  
\[
\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
\]

- Converts logits into a **probability distribution**.  
- Used in the **output layer for multi-class classification**.



   

3. **Implement Loss Functions**  
## Common Loss Functions

Loss functions measure the error between predictions and ground truth, guiding parameter updates via backpropagation.  
Below are several widely used loss functions grouped by task.

The loss function is mainly used in the training phase of the model. After each batch of training data is fed into the model, the predicted values are output through forward propagation, and then the loss function calculates the difference between the predicted values and the true values, i.e. the loss value. After obtaining the loss value, the model updates each parameter through backpropagation to reduce the loss between the true value and the predicted value, so that the predicted value generated by the model is closer to the true value, thus achieving the purpose of learning.
After the training of the model, the model has been back-propagated to make each parameter optimal. So the results obtained by using this model for prediction must be close to the real results.


### ðŸ”¹ Regression

#### 1. Mean Absolute Error (MAE)

**Formula:**  

L_{\text{MSE}}=\frac{1}{n}\sum_{i=1}^{n}\left(\hat{y}_i-y_i\right)^2


**Fallback:** `L_MSE = (1/n) * sum_{i=1..n} (y_hat_i - y_i)^2`

- More robust to outliers than MSE.  
- Gradient is less smooth (may converge slower).  


---

#### 2. Huber Loss
**Formula:**  
\[
L_\delta(a) =
\begin{cases}
\frac{1}{2}a^2 & \text{if } |a| \leq \delta \\
\delta(|a| - \tfrac{1}{2}\delta) & \text{otherwise}
\end{cases}
\]

- Combines MSE (for small errors) and MAE (for large errors).  
- Robust against outliers, smooth near zero.  



---

### ðŸ”¹ Classification

#### 1. Hinge Loss
**Formula:**  
\[
L = \sum_{i=1}^{n} \max(0, 1 - y_i \hat{y}_i)
\]

- Used in **Support Vector Machines (SVM)**.  
- Encourages a margin between classes.  



---

#### 2. Focal Loss
**Formula:**  
\[
L = - \alpha (1 - \hat{y}_i)^\gamma y_i \log(\hat{y}_i)
\]

- Emphasizes **hard-to-classify samples**.  
- Commonly used in **object detection** (e.g., RetinaNet).  


---

### ðŸ”¹ Generative Models

#### 1. KL Divergence
**Formula:**  
\[
D_{KL}(P || Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
\]

- Measures the distance between two probability distributions.  
- Widely used in **Variational Autoencoders (VAE)**.  


---

#### 2. GAN Loss (Adversarial Loss)
**Formula:**  
\[
\min_G \max_D \; \mathbb{E}_{x \sim p_{data}}[\log D(x)] +
\mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
\]

- Used in **Generative Adversarial Networks (GANs)**.  
- Generator and discriminator compete in a minimax game.  




5. **Design Layers**  
   Construct layers such as input, hidden, and output layers, and define how neurons are connected.
   ðŸ— ç¥žç»ç½‘ç»œå±‚ç¼–å†™æ¡†æž¶
## Neural Network Layers

This section introduces the core building blocks of a neural network.  
Each layer includes **parameter definition, initialization, forward propagation, backward propagation, and output shape**.

---

### 1. Fully Connected (Dense) Layer

- **Parameters**
  - Weights \( W \in (in\_features, out\_features) \)  
  - Bias \( b \in (out\_features,) \)

- **Initialization**
  - Xavier or He initialization:  
    \( W \sim \mathcal{N}(0, \sqrt{\tfrac{2}{in\_features}}) \)

- **Forward**
  \[
  Z = XW + b
  \]

- **Backward**
  \[
  dW = X^T dZ, \quad db = \sum dZ, \quad dX = dZ W^T
  \]

- **Output Shape**
  - Input: `(batch_size, in_features)`  
  - Output: `(batch_size, out_features)`

---

### 2. Convolutional (Conv2D) Layer

- **Parameters**
  - Kernel size: \( (k_h, k_w) \)  
  - Number of filters: `out_channels`  
  - Stride, Padding  

- **Initialization**
  - Weights \( W \in (out\_channels, in\_channels, k_h, k_w) \)  
  - Bias \( b \in (out\_channels,) \)

- **Forward**
  \[
  out = conv2d(X, W, stride, padding) + b
  \]

- **Backward**
  - \( dW = X * dZ \)  
  - \( db = \sum dZ \)  
  - \( dX = dZ * W^T \)  

- **Output Shape**
  \[
  H_{out} = \frac{H_{in} - k_h + 2p}{s} + 1, \quad 
  W_{out} = \frac{W_{in} - k_w + 2p}{s} + 1
  \]

---

### 3. MaxPooling Layer

- **Parameters**
  - Pool size: \( (p_h, p_w) \)  
  - Stride  

- **Initialization**
  - No trainable parameters  

- **Forward**
  - Output the maximum value in each pooling window  

- **Backward**
  - Gradient flows only to the max element in each window  

- **Output Shape**
  \[
  H_{out} = \frac{H_{in} - p_h}{s} + 1, \quad 
  W_{out} = \frac{W_{in} - p_w}{s} + 1
  \]

---

### 4. Batch Normalization Layer

- **Parameters**
  - Learnable scale \( \gamma \), shift \( \beta \)

- **Initialization**
  - \( \gamma = 1, \ \beta = 0 \)

- **Forward**
  \[
  \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}, \quad 
  y = \gamma \hat{x} + \beta
  \]

- **Backward**
  - Compute gradients for \(\gamma, \beta\) and input  

- **Output Shape**
  - Same as input

---

### 5. Dropout Layer

- **Parameters**
  - Dropout rate \( p \)

- **Initialization**
  - No trainable parameters  

- **Forward**
  - Randomly drop units:  
    \( out = X * mask / (1-p) \)

- **Backward**
  - \( dX = dOut * mask / (1-p) \)

- **Output Shape**
  - Same as input

---

### 6. Activation Layers

- **ReLU**
  - Forward: \( f(x) = \max(0, x) \)  
  - Backward: \( f'(x) = 1_{x > 0} \)

- **Sigmoid**
  - Forward: \( f(x) = \frac{1}{1+e^{-x}} \)  
  - Backward: \( f'(x) = f(x)(1-f(x)) \)

- **Tanh**
  - Forward: \( f(x) = \tanh(x) \)  
  - Backward: \( f'(x) = 1-\tanh^2(x) \)

- **Softmax**
  - Forward: \( f(x)_i = \frac{e^{x_i}}{\sum_j e^{x_j}} \)  
  - Backward: Jacobian matrix (often simplified with cross-entropy)

- **Output Shape**
  - Same as input


7. **Build Optimizers**  
   Implement optimization algorithms (e.g., Gradient Descent, Adam) to update the weights and biases efficiently.

8. **Combine and Train the Network**  
   Integrate activation functions, loss functions, layers, and optimizers into a complete neural network.  
   Train the model by iteratively forward-propagating inputs, calculating the loss, backpropagating gradients, and updating parameters.
